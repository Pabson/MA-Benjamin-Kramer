\chapter{Implementation}

In this chapter a benchmarking framework is introduced that allows comparing the
three implementations of a scan

\begin{itemize}
  \item \simdscan{} SAP proprietary using AVX~2
  \item \bwv{} custom implementation using AVX~2
  \item \bs{} custom implementation using AVX~2
\end{itemize}

\section{Benchmark Design}

The benchmark code is based on the code used to drive the benchmark
in~\cite{AVX2-Scan}. It contains the infrastructure to execute scans using
SAP's \simdscan{} and provides integration with Intel's \emph{Performance
Counter Monitor} (PCM)~\cite{intelpcm} to gather data from the various
performance counters in the CPU core. It also contains code to run and validate
a range predicate on a chunk of memory and storing the result as a bit vector,
which could be partially reused.

It was upgraded to use the latest available version of \simdscan{} from HANA
which contains additional optimizations over those described in the paper.
Support for benchmarking \bwv{} and \bs{} was added, including a new
benchmarking mode for unpacking data from the vertical layout.

\begin{description}
\item[range scan to bit vector]
  A column of uniformly distributed random 32 bit integers is generated and
  brought into the representation needed for the scanning algorithm under a
  given bit width. A range predicate of the form $0 \le x < max$ is created,
  where $max$ is chosen based on the selectivity, e.g.  a selectivity of
  $50\,\%$ with eight-bit values would result in the predicate $0 \le x < 128$.
  The scan is executed once and compared with a scalar version of the algorithm
  to ensure correctness, both algorithms produce a bit vector result. Then the
  scan is performed three times under performance counter supervision and the
  result is reported.

  This is repeated for a range of selectivities from $100\,\%$ to $0\,\%$ and
  for all bit widths from $1$ to $32$ for \simdscan{} and \bwv{}, for \bs{} only
  8, 16, 24 and 32 bits are used. The size of a column was fixed at $2^{25}$
  items.

\item[equality scan to bit vector]
  This mode is very similar to the range scan but uses a predicate of the form
  $x = c$, where $c$ is a random constant. Since the selectivity cannot be
  enforced via the predicate the input data is altered to contain $c$ at random
  positions under a uniform distribution. The number of $c$s in the result is
  chosen based on the selectivity. Otherwise the scan is carried out using the
  same methods as the range scan.

\item[multi-column scan]
  To measure the impact of block-wise scanning a multi-column mode was added
  that partitions the randomly generated input data chunk into multiple
  columns. On those columns the predicates described above are used. Columns
  can be scanned in its entirety (block size is infinite) or with smaller
  block sizes. The results are combined with a logical and operation.

  It turned out that $2^{25}$ items were insufficient to completely remove
  caching effects from the results so the number of items was later raised to
  $2^{30}$ items for all columns combined.

  Since a logical and is used to combine the results early pruning was not used
  in between columns for this benchmark. Pruning between columns with an and
  predicate becomes very sensitive to the selectivities of the individual
  columns as they multiply from column to column, distorting the results per
  column.

\item[packing]
  This benchmarking mode generates another chunk of 32 bit integers which are
  then brought into the format used by the scan algorithm. For \simdscan{}
  packing (and unpacking) is performed using the FastPFOR library by D. Lemire
  et al~\cite{fastpfor}, \bwv{} and \bs{} use
  custom packing routines. The runtime of the packing is measured with PCM.

  The packing is executed for all bit widths from 1 to 32. For \bs{} zero
  padding is added for cases not evenly divisible by eight. Not the entire
  column is unpacked but only slices. The benchmark tests multiple slice sizes
  from $256$ items to $2^{25}$ items.

\item[unpacking]
  Same as packing, but in this case integers are unpacked from the
  scan-specific representation into an array of 32 bit integers. Otherwise the
  configuration is identical to the packing benchmark.
\end{description}

\section{AVX~2 \bwv{} Implementation}

To have a fair comparison a clean-room implementation of the \bwv{} scan
algorithm was implemented in C++ using AVX~2 compiler intrinsics. Implementing
the scan itself was rather straight forward and only minor deviations from the
suggested implementation in the paper were made. The most significant change is
that the values that replicated bits of the reference constants over a full
vector were computed on demand using algorithm~\ref{algo:splatbit}. This did not
change the runtime of the scan but potentially saves resources in the already
congested load path.

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \Procedure{splatBit}{$w$, $b$}
  \State $w \gets w << (31-b)$ \Comment{move bit $b$ into the most significant position}
  \State $w \gets w >> 31$ \Comment{arithmetic shift right}
  \State \Return \_mm256\_set1\_epi32($w$) \Comment{return splat vector}
  \EndProcedure
\end{algorithmic}
\caption{Duplicate bit number $b$ from a 32 bit integer over a
full 256 bit vector}
\label{algo:splatbit}
\end{algorithm}

The most complex part of the implementation was designing a fast way to unpack
and pack the input data into the format required by the \bwv{} scan. For packing
the \texttt{movmskps} instruction was used combined with vector shifts. This
processes eight 32 bit integers at the same time but is still slow as a lot of
memory is touched. Since there is no reverse operation of \texttt{movmskps} in
the AVX~2 instruction set unpacking was implemented by replicating the bits of a
single value over a full vector and using shifts and masking.

Packing and unpacking is generally very slow for \bwv{} as unpacking a single
value has to touch as many cache lines as there are bits in the value. This is a
significant disadvantage in comparison with the other methods.

\section{AVX~2 \bs{} Implementation}

In contrast to the \bwv{} scan the \bs{} turned out to be trickier to implement
due to limitations in the AVX~2 instruction set that were not described in the
\bs{} paper. The implementation relies on unsigned integer comparisons but AVX~2
only supplies signed versions. To turn the signed versions into unsigned
compares all sign bits are flipped as illustrated in
algorithm~\ref{algo:cmpunsigned}. This adds more instructions to the main scan
loop than earlier anticipated.

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \Procedure{unsignedCompare}{bits, reference, bytenumber}
  \State bits $\gets$ \_mm256\_xor\_si256(Bits, \_mm256\_set1\_epi8(0x80));
  \State reference $\gets$ reference \^{} 0x80808080
  \State reference $\gets$ splatByte(reference, bytenumber)
  \State \Return \_mm256\_cmpgt\_epi8(bits, reference)
  \EndProcedure
\end{algorithmic}
\caption{Extract a reference byte and do an unsigned comparison with a vector}
\label{algo:cmpunsigned}
\end{algorithm}

The byte extraction and duplication is also implemented differently from the
paper. It uses the same method as the \bwv{} implementation but adapted to bytes
instead of bits (algorithm~\ref{algo:splatbyte}).

\begin{algorithm}[h]
\begin{algorithmic}[1]
  \Procedure{splatByte}{$w$, $b$}
  \State $w \gets w >> (b * 8)$ \Comment{move byte $b$ into the least significant position}
  \State \Return \_mm256\_set1\_epi8($w$) \Comment{return splat vector}
  \EndProcedure
\end{algorithmic}
\caption{Duplicate byte number $b$ from a 32 bit integer over a full 256 bit vector}
\label{algo:splatbyte}
\end{algorithm}

Packing and unpacking into the \bs{} representation can be done extremely
quickly on AVX~2. The basic idea for packing is to load a full 256 bit vector
containing eight 32 bit integers. Then use vector shuffle instructions to
rearrange the bytes of those integers to be clustered together, that means all
most significant bytes are in the first 64 bits of the vector. Then up to four
64 bit integers are extracted from that vector and stored at the corresponding
positions in the output buffer. This only touches five memory locations, reading
from one and storing to four at the maximum bit width of 32 bits. All accesses
are sequential so it should perform very well on modern hardware.

For \bs{} unpacking and packing a single value only touches eight times fewer
cache lines as \bwv{} does making the transformation process much faster.
