\chapter{Evaluation}
\label{chapter:evaluation}

After laying down the theoretical analysis of the scan algorithms and providing
an efficient implementation it is time to verify whether the expectations hold
up in reality. This chapter does so with a focus on six basic questions.

\begin{enumerate}
  \item Can the early pruning in \bwv{} beat the performance of \simdscan{}
despite of branching overhead?
  \item How large is this branching overhead?
  \item Is the early pruning probability difference of \bwv{} and \bs{} visible in practice?
  \item What is the impact of predicate selectivity on early pruning?
  \item How much time does it take to bring data in the formats required by the scanning algorithms.
  \item Are there visible improvements due to the bandwidth reduction of block-wise scanning?
\end{enumerate}

\section{Benchmark Setup}

All benchmarks were performed on a dual-CPU Intel Xeon E5-2697 v3 machine. This
Haswell setup had 56 physical and logical cores but all benchmarks only measure
single-core performance. The CPU has a last-level cache size of 35 megabytes.
Memory was supplied by eight banks with eight gigabytes of DDR4 RAM each,
running at 1067\,MHz. On the software side Ubuntu 14.04 was used and GCC 4.8
compiled the benchmark code, except for \simdscan{} which was using a binary
compiled with ICC.

\section{Comparing \simdscan{} to \bwv{}}

The first and most important evaluation result will be a direct comparison
between \simdscan{} and \bwv{}. As seen in figure~\ref{fig:eval:bwvsimdb8}. In
this configuration early pruning was performed after reading 4 bits as
suggested in~\cite{BitWeaving}.

\begin{figure}[h]
\pgfplotsset{footnotesize,width=0.75*\textwidth,height=6cm,compat=1.8}
\begin{center}
\begin{tikzpicture} \begin{axis}[
    title=\simdscan{} vs. \bwv{},
    ylabel={Runtime (ns)},
    ymin=0,%ymax=24000,
    xlabel={Bit Width},
    xmin=1,xmax=32,
    mark size=1pt,
    scaled ticks=false,
    legend columns=-1,
    legend entries={\simdscan{}, \bwv{}},
    legend to name=leg:bwvsimdb8,
    grid
]
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=6] {data/AVX2_range_noblock_b4.csv};
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_range_noblock_b4.csv};
\end{axis}
\end{tikzpicture}

\begin{tikzpicture} \begin{axis}[
    ylabel={Bytes Read from DRAM},
    ymin=0,%ymax=24000,
    xlabel={Bit Width},
    xmin=1,xmax=32,
    mark size=1pt,
    scaled ticks=false,
    grid
]
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=8] {data/AVX2_range_noblock_b4.csv};
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=8] {data/BWV_range_noblock_b4.csv};
\end{axis}
\end{tikzpicture}
\ref*{leg:bwvsimdb8}
\end{center}
\caption{Runtime of a range predicate with \simdscan{} and \bwv{}, sum of eight
runs over $2^{25}$ random integers. \bwv{} group size is four.}
\label{fig:eval:bwvsimdb8}
\end{figure}

As described earlier the benchmark was run with multiple selectivities, which
are represented as a stack of markers in this chart. Since selectivity has
virtually no impact on the runtime of \simdscan{} the variance between the
selectivities can be interpreted as an indicator for the noise level. The read
bandwidth used by \simdscan{} is approximately 11 gigabytes per second and
scales linearly over all bit widths with only a few exceptions. 11 gigabytes
per second matches the bandwidth available to a single core on the Haswell
processor used. This was verified with a separate benchmark only touching every
cache line once. Total bandwidth is significantly higher but can only made use
of by utilizing multiple cores. The graph itself can be analyzed as four
different parts.

\begin{itemize}
  \item Bit widths $0-8$: due to the large amount of last-level cache
  available not all of the memory is read from DRAM but was kept in the cache
  over multiple iterations. This skews the result, however there is virtually
  no difference in scan performance between \simdscan{} and \bwv{}. Early
  pruning can theoretically abort the scan after four or eight bits but the
  probability calculated in chapter~\ref{chapter:analysis} is very low so no
  effect is observed.
  \item Bit widths $8-12$: as calculated earlier and visible in the memory
  graph early pruning is still not effective here. The overhead of the additional
  branch is slowing down \bwv{} in comparison to \simdscan{} which does not have
  any branching in its scan loop. \simdscan{} is the better scan algorithm for
  these common bit widths.
  \item Bit widths $12-20$: Early pruning starts to become more effective and
  less memory is loaded from DRAM. The early pruning overhead is clearly
  visible and yields a slower runtime than \simdscan{} so its still preferable
  over \bwv{}.
  \item Bit width $20-32$: Early pruning is very effective and beats the
  runtime of \simdscan{} with a growing margin. The performance curve is
  entirely flat and will stay the same if the bit width is extended any further.
  \bwv{} is the preferable algorithm for this kind of scan.
\end{itemize}

All in all this benchmark shows that \bwv{} can beat \simdscan{}, but it is not
always a clear win. It provides no advantage over \simdscan{} at low bit widths
and is slower at medium bit widths. Only at relatively large bit widths early
pruning can make up its own overhead and provide an advantage that grows larger
as the bit width gets larger. This contradicts the observations of the
\bwv{} paper~\cite{BitWeaving} where \bwv{} is significantly faster than all
other methods. It is very likely that a faulty implementation of \simdscan{} was
used for those benchmarks as it is very competitive.

\section{Reducing the early pruning overhead}

As seen in the last graphs there is a significant overhead in the area around
16 bits when comparing \bwv{} with \simdscan{}. This overhead comes from the
additional branching that has to be performed at every early pruning step. To
reduce the amount of branching one possible tweak is to reduce the number of
bits that are processed without early pruning. This diminishes the
effectiveness of early pruning though. For the benchmark in
figure~\ref{fig:eval:bwvsimdbgroups} early pruning was performed after every 4,
8 or 16 bits processed. The selectivity for this benchmark was fixed at $50\,\%$.

\begin{figure}[h]
\pgfplotsset{footnotesize,width=0.75*\textwidth,height=6cm,compat=1.8}
\begin{center}
\begin{tikzpicture} \begin{axis}[
    title=\simdscan{} vs. \bwv{},
    ylabel={Runtime (ns)},
    ymin=0,%ymax=24000,
    xlabel={Bit Width},
    xmin=1,xmax=32,
    mark size=1pt,
    scaled ticks=false,
    legend columns=-1,
    legend entries={\simdscan{}, \bwv{} B=4, \bwv{} B=8, \bwv{} B=16,},
    legend to name=leg:bwvsimdbgroups,
    grid
]
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=6, each nth point=32, filter discard warning=false, unbounded coords=discard, skip rows between index={0}{2}] {data/AVX2_range_noblock_b4.csv};
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=6, each nth point=32, filter discard warning=false, unbounded coords=discard, skip rows between index={0}{2}] {data/BWV_range_noblock_b4.csv};
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=6, each nth point=32, filter discard warning=false, unbounded coords=discard, skip rows between index={0}{2}] {data/BWV_range_noblock_b8.csv};
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=6, each nth point=32, filter discard warning=false, unbounded coords=discard, skip rows between index={0}{2}] {data/BWV_range_noblock_b16.csv};
\end{axis}
\end{tikzpicture}

\begin{tikzpicture} \begin{axis}[
    ylabel={Bytes Read from DRAM},
    ymin=0,%ymax=24000,
    xlabel={Bit Width},
    xmin=1,xmax=32,
    mark size=1pt,
    scaled ticks=false,
    grid
]
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=8, each nth point=32, filter discard warning=false, unbounded coords=discard, skip rows between index={0}{2}] {data/AVX2_range_noblock_b4.csv};
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=8, each nth point=32, filter discard warning=false, unbounded coords=discard, skip rows between index={0}{2}] {data/BWV_range_noblock_b4.csv};
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=8, each nth point=32, filter discard warning=false, unbounded coords=discard, skip rows between index={0}{2}] {data/BWV_range_noblock_b8.csv};
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=8, each nth point=32, filter discard warning=false, unbounded coords=discard, skip rows between index={0}{2}] {data/BWV_range_noblock_b16.csv};
\end{axis}
\end{tikzpicture}
\ref*{leg:bwvsimdbgroups}
\end{center}
\caption{Runtime of a range predicate with \simdscan{} and \bwv{}, sum of eight
runs over $2^{25}$ random integers. Varying group sizes.}
\label{fig:eval:bwvsimdbgroups}
\end{figure}

As expected smaller group sizes work better with regard to the number of bytes
read from DRAM as early pruning is a lot more effective. The impact on
performance is very small though, the expected improvement due to fewer
mispredicted branches is not visible. Nullifying the overhead does not seem
possible.

As the difference is very small a block size of eight was chosen for the
following experiments, as it compares better to \bs{} which has an intrinsic
group size of eight due to processing values at the byte level.

\section{Comparing \bwv{} to \bs{}}

\section{Comparing predicates and selectivity}

So far only a range predicate was tested. For random data in a uniform
distribution this has the very nice property of hitting the worst case very
rarely. As a reminder, for the used predicate of the form $x\le y < z$ the worst
case is triggered when $y$ is either equal to $x$ or $z$. So for the next
benchmark this worst case will be investigated. For reasons of simplicity an
equality predicate was used, but the observation also applies to any inequality
or range predicate. Only 8, 16, 24 and 32 bits are evaluated because others are
not directly supported by \bs{}. The results in runtime and DRAM reads are
plotted in figure~\ref{fig:eval:equalbig}.

\begin{figure}[h]
\pgfplotsset{footnotesize,width=4cm,height=6cm,compat=1.8}
\begin{center}
\begin{tikzpicture} \begin{axis}[
    ylabel={Runtime (ns)},
    ymin=0,ymax=100000000,
    xlabel={Bit Width},
    xtick={8,16,24,32},
    mark size=1pt,
    scaled ticks=false,
    legend columns=-1,
    %legend entries={\simdscan{}, \bwv{}, \bs{}},
    legend to name=leg:equalbig,
    grid
]
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=6] {data/AVX2_equalbig.csv};
\addlegendentry{\simdscan{}}
% Horrible hack to populate the legend.
\addplot+[color=red] coordinates {(8,-100)};
\addlegendentry{\bwv{}}
\addplot+[color=olive] coordinates {(8,-100)};
\addlegendentry{\bs{}}
\end{axis}
\end{tikzpicture}
\begin{tikzpicture} \begin{axis}[
    ymin=0,ymax=100000000,
    yticklabels={,,},
    xlabel={Bit Width},
    xtick={8,16,24,32},
    mark size=1pt,
    scaled ticks=false,
    grid
]
\addplot+[color=red,only marks] table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_equalbig.csv};
\end{axis}
\end{tikzpicture}
\begin{tikzpicture} \begin{axis}[
    ymin=0,ymax=100000000,
    yticklabels={,,},
    xlabel={Bit Width},
    xtick={8,16,24,32},
    mark size=1pt,
    scaled ticks=false,
    grid
]
\addplot+[color=olive,only marks] table[header=false,col sep=semicolon, x index=2, y index=6] {data/BS_equalbig.csv};
\end{axis}
\end{tikzpicture}

\begin{tikzpicture} \begin{axis}[
    ylabel={Bytes Read from DRAM},
    ymin=0,ymax=1200000000,
    xlabel={Bit Width},
    xtick={8,16,24,32},
    mark size=1pt,
    scaled ticks=false,
    grid
]
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=8] {data/AVX2_equalbig.csv};
\end{axis}
\end{tikzpicture}
\begin{tikzpicture} \begin{axis}[
    ymin=0,ymax=1200000000,
    yticklabels={,,},
    xlabel={Bit Width},
    xtick={8,16,24,32},
    mark size=1pt,
    scaled ticks=false,
    grid
]
\addplot+[color=red,only marks] table[header=false,col sep=semicolon, x index=2, y index=8] {data/BWV_equalbig.csv};
\end{axis}
\end{tikzpicture}
\begin{tikzpicture} \begin{axis}[
    ymin=0,ymax=1200000000,
    yticklabels={,,},
    xlabel={Bit Width},
    xtick={8,16,24,32},
    mark size=1pt,
    scaled ticks=false,
    grid
]
\addplot+[color=olive,only marks] table[header=false,col sep=semicolon, x index=2, y index=8] {data/BS_equalbig.csv};
\end{axis}
\end{tikzpicture}

\ref*{leg:equalbig}
\end{center}
\caption{Runtime of an equality predicate with \simdscan{}, \bwv{} and \bs{},
sum of eight runs over $2^{25}$ random integers. 32 different selectivities
between $100\,\%$ and $0\,\%$.}
\label{fig:eval:equalbig}
\end{figure}

\begin{itemize}
  \item \simdscan{} is the base line. \bwv{} and \bs{} read at most as much data
    as \simdscan{}. The performance represents the maximum amount of data one
    CPU core can process at the given time, so any other algorithm can not be
    faster unless it reads less data.
  \item When the same amount of data is read the performance of \bwv{} tends to
    be almost identical to \simdscan{}. On the one hand this equality predicate
    does very little arithmetic work compared the range predicate shown earlier
    on the other hand early pruning does not work well so the early exit branch
    can be easily predicted by the hardware.
  \item Early pruning is more effective for \bs{} than for \bwv{} but
    performance tends to be slightly worse. The implementation of \bs{} is not
    entirely memory bound like \simdscan{} and \bwv{} are.
\end{itemize}

One important fact that is not represented in this graph is that for \bwv{} and
\bs{} about half of the points are in the topmost position. This is shown in
detail in figure~\ref{fig:eval:equalbigselectivity} which shows the results
against the selectivity on a logarithmic scale. The results are only shown for a
bit width of 32 as this shows early pruning in action for both \bwv{} and \bs{}.

\begin{figure}[h]
\pgfplotsset{footnotesize,width=0.75*\textwidth,height=6cm,compat=1.8}
\pgfplotsset{select coords between index/.style 2 args={
    x filter/.code={
        \ifnum\coordindex<#1\def\pgfmathresult{}\fi
        \ifnum\coordindex>#2\def\pgfmathresult{}\fi
    }
}}
\begin{center}

\begin{tikzpicture} \begin{semilogxaxis}[
    ylabel={Runtime (ns)},
    ymin=0,ymax=100000000,
    xlabel={Selectivity},
    mark size=1pt,
    scaled ticks=false,
    legend columns=-1,
    legend entries={\simdscan{}, \bwv{}, \bs{}},
    legend to name=leg:equalbigselectivity,
    grid
]
\addplot+ [select coords between index={0}{32}] table[header=false,col sep=semicolon, x index=5, y index=6,filter discard warning=false] {data/AVX2_equalbig.csv};
\addplot+ [select coords between index={0}{32}] table[header=false,col sep=semicolon, x index=5, y index=6,filter discard warning=false] {data/BWV_equalbig.csv};
\addplot+ [select coords between index={0}{32}] table[header=false,col sep=semicolon, x index=5, y index=6,filter discard warning=false] {data/BS_equalbig.csv};
\end{semilogxaxis}
\end{tikzpicture}

\begin{tikzpicture} \begin{semilogxaxis}[
    ylabel={Bytes Read from DRAM},
    ymin=0,ymax=1200000000,
    xlabel={Selectivity},
    mark size=1pt,
    scaled ticks=false,
    grid
]
\addplot+ [select coords between index={0}{32}] table[header=false,col sep=semicolon, x index=5, y index=8,filter discard warning=false] {data/AVX2_equalbig.csv};
\addplot+ [select coords between index={0}{32}] table[header=false,col sep=semicolon, x index=5, y index=8,filter discard warning=false] {data/BWV_equalbig.csv};
\addplot+ [select coords between index={0}{32}] table[header=false,col sep=semicolon, x index=5, y index=8,filter discard warning=false] {data/BS_equalbig.csv};
\end{semilogxaxis}
\end{tikzpicture}

\ref*{leg:equalbigselectivity}
\end{center}
\caption{Different selectivities for \simdscan{}, \bwv{} and \bs{} for code
width 32.}
\label{fig:eval:equalbigselectivity}
\end{figure}

The graph clearly shows the early pruning patterns calculated in
chapter~\ref{chapter:analysis}. The bit width of 32 bits is also rather large so
early pruning can work decently here and the branch overhead does not penalize
\bwv{} in comparison to \simdscan{}. For \bs{} the situation looks a bit
different though as there are obvious deficiencies in performance here. It is
unclear why there is a performance advantage over \bwv{} and \simdscan{} for
very high selectivities.

\section{Packing and Unpacking}

Packing the data into the representation required by the scan algorithm and
unpacking them for consumption are two operations that are often overlooked when
measuring scan performance. While packing performance is usually done once for a
column which is scanned many times, unpacking is necessary to materialize the
results. This can be circumvented by keeping the data available in multiple
formats, one optimized for scanning and another optimized for unpacking, but
this obviously requires twice the amount of memory.
Figure~\ref{fig:eval:packunpack} shows packing of 32 bit values into the scan
representation of various sizes and the reverse, unpacking the scan
representation back into 32 bit values. For \simdscan{} at bit width 32 both
operations are equivalent to \texttt{memcpy}, performing a verbatim copy and not
modifying the data at all.

\begin{figure}[h]
\pgfplotsset{footnotesize,width=0.75*\textwidth,height=6cm,compat=1.8}
\begin{center}
\begin{tikzpicture} \begin{axis}[
    title={Packing},
    ylabel={Runtime (ns)},
    ymin=0,%ymax=24000,
    xlabel={Bit Width},
    xmin=1,xmax=32,
    mark size=1pt,
    scaled ticks=false,
    legend columns=-1,
    legend entries={\simdscan{}, \bwv{}, \bs{}},
    legend to name=leg:packunpack,
    grid
]
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=6] {data/AVX2_pack.csv};
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_pack.csv};
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=6] {data/BS_pack.csv};
\end{axis}
\end{tikzpicture}

\begin{tikzpicture} \begin{axis}[
    title={Unpacking},
    ylabel={Runtime (ns)},
    ymin=0,%ymax=24000,
    xlabel={Bit Width},
    xmin=1,xmax=32,
    mark size=1pt,
    scaled ticks=false,
    grid
]
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=6] {data/AVX2_unpack.csv};
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_unpack.csv};
\addplot+[only marks] table[header=false,col sep=semicolon, x index=2, y index=6] {data/BS_unpack.csv};
\end{axis}
\end{tikzpicture}
\ref*{leg:packunpack}
\end{center}
\caption{Packing 32 bit values into \simdscan{}, \bwv{} and \bs{}
representations and the reverse. 256 values were transformed at a time.}
\label{fig:eval:packunpack}
\end{figure}

The most obvious issue is that \bwv{} is orders of magnitudes slower than the
other two representations. This is due to the amount of arithmetic needed to
extract and insert individual bits while manipulating fully bytes or more can be
done much more efficiently with shifts and vector shuffles. It should also be
noted that packing is more expensive than unpacking in this scenario.

For \simdscan{} and \bs{} runtime is extremely close to the speed of memory as
seen in the 32 bit case of packing. The difference in performance between the
two is mostly due to the \bs{} packing algorithm making use of SIMD while the
others were implemented using scalar 32 or 64 bit integers. There is still more
potential for optimization here.

In conclusion it is certainly feasible to use the in-memory representation of
\simdscan{} or \bs{} as the storage format for both scanning and extraction of
values, unpacking from \bwv{} is prohibitively expensive so its usage is
restricted to being an index.

\section{Adding Block-Wise Scanning}

The last part of this evaluation is measuring the impact of block-wise scanning
on the used memory bandwidth, and thus performance. Five block sizes were
evaluated, only \bwv{} results are shown here as \simdscan{} and \bs{} show
exactly the same behavior.

\begin{itemize}
  \item Block size $1$. This is equivalent to a row-wise scan. One element is read
    from the first column, compared and then the next column is processed before
    eventually advancing to the next row. For this test a scalar loop was used
    as the other methods have minimum block sizes because of their SIMD usage.
  \item Block size $256$. This is the native width of \bwv{} using AVX~2.
  \item Block size $1024$. Picked to see if using the native width has an
    positive or negative impact.
  \item Block size $2^{17}=131\,072$. This was used to emulate a database system
    that divides its data into chunks of $100\,000$ elements. Rounded up to the
    next power of 2.
  \item Block size $\infty$. This is equivalent to scanning the entire column
    before advancing to the next column.
\end{itemize}

Except for the infinite block size all intermediate bit vectors always fit in
cache and since the bit vectors are permanently overwritten it is very unlikely
for them to be evicted from cache.

First a word on block size $1$. Tests showed that this method was not
competitive at all with the other sizes because it inhibits using SIMD
instructions. Testing showed a slow down of more than $10\times$. Using this
method is not recommended for any type of scan.

The next graph in figure~\ref{fig:eval:bwvblocks4} compares block sizes $256$
and $1024$ with an infinite block size. There is a small performance benefit of
this method but it is not very pronounced, as calculated in
chapter~\ref{chapter:analysis}. What does not match the analysis is that the
bit width has virtually no effect on the performance difference even though the
percentage of saved memory bandwidth is much higher for small bit widths. This
effect is most likely due to cache effects.

\begin{figure}[h]
\pgfplotsset{footnotesize,width=0.75*\textwidth,height=6cm,compat=1.8}
\begin{center}
\begin{tikzpicture} \begin{axis}[
    title=\bwv{} block-wise 4 column,
    ylabel={Runtime (ns)},
    ymin=0,%ymax=24000,
    xlabel={Bit Width},
    xmin=1,xmax=32,
    mark size=1pt,
    scaled ticks=false,
    legend columns=-1,
    legend entries={\bwv{} 256, \bwv{} 1024, \bwv{} $\infty$},
    legend to name=leg:bwvblocks4,
    grid
]
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_blocked_big_256_4col.csv};
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_blocked_big_1024_4col.csv};
%\addplot+ table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_blocked_big_100000_4col.csv};
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_blocked_big_inf_4col.csv};
\end{axis}
\end{tikzpicture}

\begin{tikzpicture} \begin{axis}[
    ylabel={Bytes Written to DRAM},
    ymin=0,%ymax=24000,
    xlabel={Bit Width},
    xmin=1,xmax=32,
    mark size=1pt,
    scaled ticks=false,
    grid
]
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=9] {data/BWV_blocked_big_256_4col.csv};
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=9] {data/BWV_blocked_big_1024_4col.csv};
%\addplot+ table[header=false,col sep=semicolon, x index=2, y index=9] {data/BWV_blocked_big_100000_4col.csv};
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=9] {data/BWV_blocked_big_inf_4col.csv};
\end{axis}
\end{tikzpicture}
\ref*{leg:bwvblocks4}
\end{center}
\caption{Comparing different block sizes for four columns with \bwv{}}
\label{fig:eval:bwvblocks4}
\end{figure}

Figure~\ref{fig:eval:bwvblocks16} shows the same scan but this time the data was
distributed over 16 columns. This has a catastrophic effect on scan performance
because the memory prefetching of the used Haswell CPU cannot cope with this
many streams, effectively removing the performance advantage of sequential
access. The amount of written data is much lower for this test case though, the
amount of input data was the same overall but more of the results were
intermediate ones compared to the four column case. The intermediate results
fit in cache for small bit widths here leading to a curved graph for the
infinite block size, this will not happen for larger data sets.

\begin{figure}[h]
\pgfplotsset{footnotesize,width=0.75*\textwidth,height=6cm,compat=1.8}
\begin{center}
\begin{tikzpicture} \begin{axis}[
    title=\bwv{} block-wise 16 column,
    ylabel={Runtime (ns)},
    ymin=0,%ymax=24000,
    xlabel={Bit Width},
    xmin=1,xmax=32,
    mark size=1pt,
    scaled ticks=false,
    legend columns=-1,
    legend entries={\bwv{} 256, \bwv{} 1024, \bwv{} $\infty$},
    legend to name=leg:bwvblocks16,
    grid
]
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_blocked_big_256_16col.csv};
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_blocked_big_1024_16col.csv};
%\addplot+ table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_blocked_big_100000_16col.csv};
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_blocked_big_inf_16col.csv};
\end{axis}
\end{tikzpicture}

\begin{tikzpicture} \begin{axis}[
    ylabel={Bytes Written to DRAM},
    ymin=0,%ymax=24000,
    xlabel={Bit Width},
    xmin=1,xmax=32,
    mark size=1pt,
    scaled ticks=false,
    grid
]
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=9] {data/BWV_blocked_big_256_16col.csv};
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=9] {data/BWV_blocked_big_1024_16col.csv};
%\addplot+ table[header=false,col sep=semicolon, x index=2, y index=9] {data/BWV_blocked_big_100000_16col.csv};
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=9] {data/BWV_blocked_big_inf_16col.csv};
\end{axis}
\end{tikzpicture}
\ref*{leg:bwvblocks16}
\end{center}
\caption{Comparing different block sizes for sixteen columns with \bwv{}}
\label{fig:eval:bwvblocks16}
\end{figure}

Once the prefetcher ceased to work there is also a visible advantage of using
larger block sizes, in this case $1024$ performed significantly better than
$256$ as there is some sequential access left that the CPU can recognize. This
lead to the next test that uses a vastly larger block size of $2^{17}$ which is
tested in figure~\ref{fig:eval:bwvblocks100k} with a larger data set.

\begin{figure}[h]
\pgfplotsset{footnotesize,width=0.75*\textwidth,height=6cm,compat=1.8}
\begin{center}
\begin{tikzpicture} \begin{axis}[
    title=\bwv{} block-wise with large block size,
    ylabel={Runtime (ns)},
    ymin=0,%ymax=24000,
    xlabel={Bit Width},
    xmin=1,xmax=32,
    mark size=1pt,
    scaled ticks=false,
    legend columns=-1,
    legend entries={\bwv{} $2^{17}$ 4 cols, \bwv{} $2^{17}$ 16 cols, \bwv{} $\infty$ 4 cols},
    legend to name=leg:bwvblocks100k,
    grid
]
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_blocked_big_100000_4col.csv};
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_blocked_big_100000_16col.csv};
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=6] {data/BWV_blocked_inf_4col.csv};
\end{axis}
\end{tikzpicture}

\begin{tikzpicture} \begin{axis}[
    ylabel={Bytes Written to DRAM},
    ymin=0,%ymax=24000,
    xlabel={Bit Width},
    xmin=1,xmax=32,
    mark size=1pt,
    scaled ticks=false,
    grid
]
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=9] {data/BWV_blocked_big_100000_4col.csv};
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=9] {data/BWV_blocked_big_100000_16col.csv};
\addplot+ table[header=false,col sep=semicolon, x index=2, y index=9] {data/BWV_blocked_inf_4col.csv};
\end{axis}
\end{tikzpicture}
\ref*{leg:bwvblocks100k}
\end{center}
\caption{Comparing \bwv{} at block size $2^{17}$}
\label{fig:eval:bwvblocks100k}
\end{figure}

With a block size this large the prefetcher issue of smaller block sizes is
completely avoided and we gain a small performance benefit by avoiding to write
intermediate results into main memory. The result for one block only weighs
$\frac{2^{17}}{8}=2^{14}$ bytes which easily fits into any cache on the
processor.

\section{Summary}

Coming back to the questions posed in the introduction of this chapter we can
now safely answer all of them.

\begin{enumerate}
  \item \emph{Can the early pruning in \bwv{} beat the performance of
    \simdscan{} despite of branching overhead?} Yes, but not for all bit widths.
    On the tested machine the branching overhead is largest around a bit width
    of 16 and amortizes if the bit width gets larger. For bit widths around 32
    bits \bwv{} has a significant advantage over \simdscan{}.
  \item \emph{How large is this branching overhead?} Large enough to make \bwv{}
    slower than \simdscan{} for some bit widths.
  \item \emph{Is the early pruning probability difference of \bwv{} and \bs{}
    visible in practice?} It is visible when measuring the amount of memory read
    from DRAM but in this implementation it was not faster than \bwv{}. \bs{}
    has other advantages over \bwv{} and it is not unlikely that \bs{} can gain
    ground on \bwv{} on the next generation of Intel hardware or with a better
    tuned implementation.
  \item \emph{What is the impact of predicate selectivity on early pruning?} For
    uniform random data there is virtually no impact for predicates containing
    less-than or greater-than operations. For equality the selectivity directly
    impacts early pruning and even at low selectivities early pruning is often
    defeated entirely.
  \item \emph{How much time does it take to bring data in the formats required
    by the scanning algorithms.} Both \simdscan{} and \bs{} really shine in this
    area. \bwv{} is orders of magnitudes slower, making it infeasible as the
    primary representation; making it only usable as an index.
  \item \emph{Are there visible improvements due to the bandwidth reduction of
    block-wise scanning?} Yes. There is a small but visible effect. Making the
    block size too small can regress performance though so a large block size is
    better as long as the intermediate bit vectors comfortably fit in cache.
\end{enumerate}
